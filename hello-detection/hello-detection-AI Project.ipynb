{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01412caf",
   "metadata": {},
   "source": [
    "# Hello Object Detection\n",
    "\n",
    "A very basic introduction to using object detection models with OpenVINO™.\n",
    "\n",
    "The [horizontal-text-detection-0001](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/horizontal-text-detection-0001/README.md) model from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) is used. It detects horizontal text in images and returns a blob of data in the shape of `[100, 5]`. Each detected text box is stored in the `[x_min, y_min, x_max, y_max, conf]` format, where the\n",
    "`(x_min, y_min)` are the coordinates of the top left bounding box corner, `(x_max, y_max)` are the coordinates of the bottom right bounding box corner and `conf` is the confidence for the predicted class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "740bfdd8",
   "metadata": {},
   "source": [
    "## Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt #이미지 출력\n",
    "from pathlib import Path #디렉토리 파일 불러오기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "151c5b81-2cf9-41a7-95ec-8170a33de965",
   "metadata": {},
   "source": [
    "## Select inference device\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674a148-58a6-4717-b0ef-73f7caa83956",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#device = device_widget()\n",
    "#device\n",
    "\n",
    "#위의 값을 대체\n",
    "core=ov.Core()\n",
    "options=core.available_devices\n",
    "options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b48949",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99737c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model='model/horizontal-text-detection-0001.xml') #모델의 위치를 재설정\n",
    "compiled_model = core.compile_model(model=model, device_name='CPU')\n",
    "## device에서 선택했던 것을 사용하는 것\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(\"boxes\")\n",
    "\n",
    "print(input_layer.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "705ce668",
   "metadata": {},
   "source": [
    "## Load an Image\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection models expect an image in BGR format.\n",
    "image = cv2.imread('data/abc.jpg')\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width.\n",
    "N, C, H, W = input_layer.shape\n",
    "\n",
    "# Resize the image to meet network expected input sizes.\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "# 이미지는 칼라면 뒤에 3이 붙는다. 흑백이면 0)\n",
    "print(resized_image.shape)\n",
    "\n",
    "# Reshape to the network input shape.\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "print(input_image.shape)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9fcaba9",
   "metadata": {},
   "source": [
    "## Do Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ca630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference request.\n",
    "boxes = compiled_model([input_image])[output_layer]\n",
    "print(boxes)\n",
    "print(\"----------------------\")\n",
    "# Remove zero only boxes.\n",
    "boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "#이미지 박스 쳐져 있는 부분, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09dfac5d",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each detection, the description is in the [x_min, y_min, x_max, y_max, conf] format:\n",
    "# The image passed here is in BGR format with changed width and height. To display it in colors expected by matplotlib, use cvtColor function\n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.3, conf_labels=True): # threshold 30% 확률로 결정해라.\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "    #RGB이기에 255=R\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "        bgr_image.shape[:2],     #517, 690\n",
    "        resized_image.shape[:2], #704, 704\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB) #bgr_image : 원래 이미지\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for box in boxes: #for은 반복문\n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                (int(max(corner_position * ratio_y, 10)) if idx % 2 else int(corner_position * ratio_x)) for idx, corner_position in enumerate(box[:-1])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3) # 박스를 그릴떄, 왼쪽 최댓값, 오른쪽 최댓값, 3은 여기서 두께\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14476f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.axis(\"off\") #죄표값 \n",
    "plt.imshow(convert_result_to_image(image, resized_image, boxes, conf_labels=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b68b0-4f7e-4018-be30-17afb17ab926",
   "metadata": {},
   "source": [
    "# 6.배포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc63731-c1d7-4c33-b831-24bcb678ccf0",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8d5ff1-7a21-428c-8710-582498363d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path #디렉토리 파일 불러오기\n",
    "import PIL\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13b7f92-7874-4078-a105-145fd2404592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPU', 'GPU']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core=ov.Core()\n",
    "options=core.available_devices\n",
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8097ddf4-6ab0-47f0-af8b-01f9678876f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,3,704,704]\n"
     ]
    }
   ],
   "source": [
    "model = core.read_model(model='model/horizontal-text-detection-0001.xml') #모델의 위치를 재설정\n",
    "compiled_model = core.compile_model(model=model, device_name='CPU')\n",
    "## device에서 선택했던 것을 사용하는 것\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(\"boxes\")\n",
    "\n",
    "print(input_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb75556-96db-41d4-9133-5aa610fa56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    N, C, H, W = input_layer.shape\n",
    "    resized_image = cv2.resize(image, (W, H))\n",
    "    input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "    \n",
    "    return input_image, resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89bef10f-0ba8-40c4-b281-df4b42b479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "        bgr_image.shape[:2],       # 517, 690\n",
    "        resized_image.shape[:2],   #704, 704\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    # rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for box in boxes:\n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                (int(max(corner_position * ratio_y, 10)) if idx % 2 else int(corner_position * ratio_x)) for idx, corner_position in enumerate(box[:-1])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(bgr_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "186b6dd7-ef9d-41e9-9fb5-1bbee74e60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    input_image, resized_image = preprocess(image)  #이미지 shape 전처리 \n",
    "    \n",
    "    boxes = compiled_model([input_image])[output_layer]\n",
    "    boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "    canvas = convert_result_to_image(image, resized_image, boxes, conf_labels=True)\n",
    "    \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b4055bc-7daf-4776-9537-cc5d10aed035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = cv2.imread('data/abc.jpg')\n",
    "#plt.imshow(predict_image(image));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa1c28d-7c50-48db-8841-870913d26b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(predict_image, gr.Image(), \"image\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20189386-b056-4b35-89c3-0fe08bdefba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/36741649/128489933-bf215a3f-06fa-4918-8833-cb0bf9fb1cc7.jpg",
   "tags": {
    "categories": [
     "First Steps"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Object Detection"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
